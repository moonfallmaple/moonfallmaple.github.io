<!DOCTYPE HTML>
<html>

<head>
	<link rel="bookmark"  type="image/x-icon"  href="/img/logo.png"/>
	<link rel="shortcut icon" href="/img/logo.png">
	
			    <title>
    月落丹枫
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="/css/mic_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
    <meta name="keywords" content="" />
    
    	<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	 
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('/img/bg.jpg') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
</head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_prism_okaidia.css" />
<link rel="stylesheet" href="/css/typo.css" />
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">月落丹枫</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow">Home</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <li class="active">
	            <a href="#s1">Category</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="category-link" href="/categories/Data-Analysis/">Data-Analysis</a>
	                    </ul>
	        </li>
	        
	        <!-- archives  归档   --> 
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/tags/" title="Tag">
		                Tag
		            </a>
		        </li>
		        
		        <li>
		            <a href="/demo/" title="Demo">
		                Demo
		            </a>
		        </li>
		        
		        <li>
		            <a href="https://www.linkedin.com/in/jane-shan-a50170142/" title="Resume">
		                Resume
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="https://github.com/moonfallmaple" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main" >
            <div class ="post_page_title_img" style="height:8rem;background-image: url();background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 2rem 4rem 2rem 4rem ;"><h2 >Credit_Card_Fraud</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 2rem 4rem 2rem 4rem ;">
                <h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>This report will test different methods on skewed data (Credit_Card_Fraud dataset). The idea is to compare if sampling techniques work better when there is an overwhelming majority class that can disrupt the efficiency of our predictive model..</p>
<p>The dataset contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.</p>
<blockquote>
<p>It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data.</p>
</blockquote>
<blockquote>
<p>Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are ‘Time’ and ‘Amount’.</p>
</blockquote>
<blockquote>
<p>Feature ‘Time’ contains the seconds elapsed between each transaction and the first transaction in the dataset.</p>
</blockquote>
<blockquote>
<p>Feature ‘Amount’ is the transaction Amount, this feature can be used for example-dependant cost-senstive learning.</p>
</blockquote>
<blockquote>
<p>Feature ‘Class’ is the response variable and it takes value 1 in case of fraud and 0 otherwise.</p>
</blockquote>
<h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><blockquote>
<p>1: Understanding our data</p>
</blockquote>
<blockquote>
<p>2:Feature Engineering</p>
</blockquote>
<blockquote>
<p>2.1: Creating ‘Hour’ Feature</p>
</blockquote>
<blockquote>
<p>2.2: Exploring patterns difference between Normal and Fraud Transanctions</p>
</blockquote>
<blockquote>
<p>2.3: Exploring feature distribution difference between Normal and Fraud transactions</p>
</blockquote>
<blockquote>
<p>2.4: Generate a new dataframe “data_new”</p>
</blockquote>
<blockquote>
<p>2.5: Feature scaling with ‘Hour’ and ‘Amount’</p>
</blockquote>
<blockquote>
<p>2.6: Exploring feature importance</p>
</blockquote>
<blockquote>
<p>3: Splitting the Data (Original DataFrame)</p>
</blockquote>
<blockquote>
<p>4: Random Under-Sampling</p>
</blockquote>
<blockquote>
<p>5: Logistic regression classifier - Skewed data</p>
</blockquote>
<blockquote>
<p>6: Logistic regression classifier - Undersampled data</p>
</blockquote>
<blockquote>
<p>7: Logistic regression classifier - use Undersampled data for fitting and original test data for testing</p>
</blockquote>
<h3 id="1-Understanding-our-data"><a href="#1-Understanding-our-data" class="headerlink" title="1: Understanding our data"></a>1: Understanding our data</h3><blockquote>
<p>Summary:</p>
</blockquote>
<blockquote>
<p>The transaction amount is relatively small. The mean of all the mounts made is approximately USD 88.</p>
</blockquote>
<blockquote>
<p>Detect missing value: There are no “Null” values, so we don’t have to work on ways to replace values.</p>
</blockquote>
<blockquote>
<p>Most of the transactions were Non-Fraud (99.83%) of the time, while Fraud transactions occurs (017%) of the time in the dataframe</p>
</blockquote>
<h5 id="Distribution-of-Class"><a href="#Distribution-of-Class" class="headerlink" title="Distribution of Class"></a>Distribution of Class</h5><div align="center"><br><img src="./q1.png" width="843" height="424" alt="图片名称" align="center"><br></div>

<h4 id="Distribution-of-Amount-and-Time"><a href="#Distribution-of-Amount-and-Time" class="headerlink" title="Distribution of Amount and Time"></a>Distribution of Amount and Time</h4><div align="center"><br><img src="./q11.png" width="847" height="334" alt="图片名称" align="center"><br></div>

<h4 id="Load-pakage-and-dataset"><a href="#Load-pakage-and-dataset" class="headerlink" title="Load pakage and dataset"></a>Load pakage and dataset</h4><pre><code class="bash">import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec

# Classifier Libraries
from sklearn.linear_model import LogisticRegression

# Classifier Evaluation
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score,
accuracy_score, classification_report
from sklearn.metrics import confusion_matrix

data=pd.read_csv(&#39;creditcard.csv&#39;)
data.tail(10)
</code></pre>
<h4 id="Visualization-of-Class-Time-and-Amount-distribution"><a href="#Visualization-of-Class-Time-and-Amount-distribution" class="headerlink" title="Visualization of (Class, Time and Amount) distribution"></a>Visualization of (Class, Time and Amount) distribution</h4><pre><code class="bash">#Distribution of Class
print(&#39;Normal transactions:&#39;,round(data[&#39;Class&#39;].value_counts()[0]/len(data[&#39;Class&#39;])*100,2),
&#39;of the dataset&#39;)
print(&#39;Fraud transactions:&#39;,round(data[&#39;Class&#39;].value_counts()[1]/len(data[&#39;Class&#39;])*100,2),
&#39;of the dataset&#39;)

fig, axs = plt.subplots(1,2,figsize=(14,7))
axs[0].set_title(&quot;Frequency of each Class&quot;)
data[&#39;Class&#39;].value_counts().plot(kind=&#39;bar&#39;,ax=axs[0])

axs[1].set_title(&quot;Percentage of each Class&quot;)
data[&#39;Class&#39;].value_counts().plot(kind=&#39;pie&#39;,ax=axs[1])
plt.show()

#Distribution of Amount and Time
fig,ax= plt.subplots(1,2,figsize=(14,5))
sns.distplot(data[&#39;Amount&#39;],ax=ax[0],color=&#39;r&#39;)
ax[0].set_title(&#39;Distribution of Transaction Amount&#39;, fontsize=14)
ax[0].set_xlim([min(data[&#39;Amount&#39;]), max(data[&#39;Amount&#39;])])

sns.distplot(data[&#39;Time&#39;],ax=ax[1])
ax[1].set_title(&#39;Distribution of Transaction Time&#39;, fontsize=14)
ax[1].set_xlim([min(data[&#39;Time&#39;]), max(data[&#39;Time&#39;])])
plt.show()
</code></pre>
<h3 id="2-Feature-Engineering"><a href="#2-Feature-Engineering" class="headerlink" title="2.Feature Engineering"></a>2.Feature Engineering</h3><h4 id="2-1-Creating-‘Hour’-Feature"><a href="#2-1-Creating-‘Hour’-Feature" class="headerlink" title="2.1: Creating ‘Hour’ Feature"></a>2.1: Creating ‘Hour’ Feature</h4><pre><code class="bash">data[&#39;Hour&#39;]=data[&quot;Time&quot;].apply(lambda x : divmod(x, 3600)[0])

</code></pre>
<h4 id="2-2-Exploring-patterns-difference-between-Normal-and-Fraud-Transanctions"><a href="#2-2-Exploring-patterns-difference-between-Normal-and-Fraud-Transanctions" class="headerlink" title="2.2:Exploring patterns difference between Normal and Fraud Transanctions"></a>2.2:Exploring patterns difference between Normal and Fraud Transanctions</h4><blockquote>
<p>Difference 1: For the fraud transactions: the correlation between some of the variables is more pronounced. The variation between V1, V2, V3, V4, V5, V6, V7, V9, V10, V11, V12, V14, V16, V17 and V18 and V19 presents a certain pattern</p>
</blockquote>
<div align="center"><br><img src="./q2.png" width="906" height="594" alt="图片名称" align="center"><br></div>

<pre><code class="bash">Xfraud = data.loc[data[&quot;Class&quot;] == 1] # update Xfraud &amp; XnonFraud with cleaned data
XnonFraud = data.loc[data[&quot;Class&quot;] == 0]

correlationNonFraud = XnonFraud.loc[:, data.columns != &#39;Class&#39;].corr()

mask = np.zeros_like(correlationNonFraud) # boolean array
indices = np.triu_indices_from(correlationNonFraud) #Return the indices for the
upper-triangle of arr.
mask[indices] = True
grid_kws = {&quot;width_ratios&quot;: (.9, .9, .05), &quot;wspace&quot;: 0.2}
f, (ax1, ax2, cbar_ax) = plt.subplots(1, 3, gridspec_kw=grid_kws, figsize = (14, 9))

cmap = sns.diverging_palette(220, 8, as_cmap=True) #color map
ax1 =sns.heatmap(correlationNonFraud, ax = ax1, vmin = -1, vmax = 1, cmap = cmap,
square = False, linewidths = 0.5, mask = mask, cbar = False)
ax1.set_xticklabels(ax1.get_xticklabels(), size = 16);
ax1.set_yticklabels(ax1.get_yticklabels(), size = 16);
ax1.set_title(&#39;Normal&#39;, size = 20)


correlationFraud = Xfraud.loc[:, data.columns != &#39;Class&#39;].corr()
ax2 = sns.heatmap(correlationFraud, vmin = -1, vmax = 1, cmap = cmap, \
      ax = ax2, square = False, linewidths = 0.5, mask = mask, yticklabels = False, \
      cbar_ax = cbar_ax, cbar_kws={&#39;orientation&#39;: &#39;vertical&#39;, \
                                 &#39;ticks&#39;: [-1, -0.5, 0, 0.5, 1]})
ax2.set_xticklabels(ax2.get_xticklabels(), size = 16);
ax2.set_title(&#39;Fraud&#39;, size = 20);

cbar_ax.set_yticklabels(cbar_ax.get_yticklabels(), size = 14)
</code></pre>
<blockquote>
<p>Difference 2: Fraud transactions tend to be Small Amount</p>
</blockquote>
<div align="center"><br><img src="./q22.png" width="848" height="333" alt="图片名称" align="center"><br></div>

<pre><code class="bash">fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, sharey=True,figsize = (14, 5))

sns.distplot(data[data[&#39;Class&#39;]==1][&#39;Amount&#39;], ax=ax0, bins=30)
ax0.set(title=&quot;Fraud&quot;,xlabel=&#39;Amount ($)&#39;,ylabel=&#39;Probability&#39;)

sns.distplot(data[data[&#39;Class&#39;]==0][&#39;Amount&#39;], ax=ax1, bins=30)
ax1.set(title=&quot;Normal&quot;,xlabel=&#39;Amount ($)&#39;,ylabel=&#39;Probability&#39;)

plt.show()
</code></pre>
<blockquote>
<p>Difference 3: The most frequent fraud transactions were happended at 11:am on the first day. The rest of Fraud transactions were happened between 11pm-9am. Indicating that the credit theft don’t want to attract the credit card owner’s attention, so they prefer to choose the time when the owner sleep and the time when consumption frequency is high.</p>
</blockquote>
<div align="center"><br><img src="./q23.png" width="1302" height="424" alt="图片名称" align="center"><br></div>

<pre><code class="bash"># High Consumption frequency: between 9:00 am to 11:00 pm
sns.factorplot(x=&#39;Hour&#39;,data=data,kind=&quot;count&quot;,  palette=&quot;ocean&quot;, size=6, aspect=3)
</code></pre>
<div align="center"><br><img src="./q24.png" width="956" height="333" alt="图片名称" align="center"><br></div>

<pre><code class="bash">fig,(ax0, ax1) = plt.subplots(nrows=1, ncols=2,figsize = (16, 5))
ax0.scatter(data[data[&#39;Class&#39;]==1][&#39;Hour&#39;],data[data[&#39;Class&#39;]==1][&#39;Amount&#39;])
ax0.set(title=&#39;Fraud&#39;,xlabel=&#39;Hour&#39;,ylabel=&#39;Amount($)&#39;)

ax1.scatter(data[data[&#39;Class&#39;]==0][&#39;Hour&#39;],data[data[&#39;Class&#39;]==0][&#39;Amount&#39;])
ax1.set(title=&#39;Normal&#39;,xlabel=&#39;Hour&#39;,ylabel=&#39;Amount($)&#39;)

plt.show()
</code></pre>
<h4 id="2-3-Exploring-feature-distribution-difference-between-Normal-and-Fraud-transactions"><a href="#2-3-Exploring-feature-distribution-difference-between-Normal-and-Fraud-transactions" class="headerlink" title="2.3: Exploring feature distribution difference between Normal and Fraud transactions"></a>2.3: Exploring feature distribution difference between Normal and Fraud transactions</h4><p>Figures below present the distribution of different variables between Normal and Fraud,we should choose those variables which has significant difference.<br>We drop variables:’V8’, ‘V13’, ‘V15’, ‘V20’, ‘V21’, ‘V22’, ‘V23’, ‘V24’, ‘V25’, ‘V26’, ‘V27’, ‘V28’</p>
<div align="center"><br><img src="./q25.png" width="939" height="1678" alt="图片名称" align="center"><br></div>

<pre><code class="bash">features_name=data.iloc[:,1:29].columns
features_name

# define the figsize of the whole picture
plt.figure(figsize=(16,30))

# define the num of columns and rows
gs = gridspec.GridSpec(7, 4)

for i,f in enumerate(data[features_name]):
    ax = plt.subplot(gs[i])
    sns.distplot(data[f][data[&quot;Class&quot;] == 1], bins=50)
    sns.distplot(data[f][data[&quot;Class&quot;] == 0], bins=50)
    ax.set_xlabel(&#39;&#39;)
    ax.set_title(&#39;histogram of feature: &#39; + str(f))

</code></pre>
<h4 id="2-4-Generate-a-new-dataframe-“data-new”"><a href="#2-4-Generate-a-new-dataframe-“data-new”" class="headerlink" title="2.4: Generate a new dataframe “data_new”"></a>2.4: Generate a new dataframe “data_new”</h4><pre><code class="bash">droplist = [&#39;V8&#39;, &#39;V13&#39;, &#39;V15&#39;, &#39;V20&#39;, &#39;V21&#39;, &#39;V22&#39;, &#39;V23&#39;, &#39;V24&#39;, &#39;V25&#39;, &#39;V26&#39;, &#39;V27&#39;, &#39;V28&#39;,&#39;Time&#39;]
data_new = data.drop(droplist, axis = 1)
data_new.shape # 查看数据的维度
data_new.head()
</code></pre>
<h4 id="2-5-Feature-scaling-with-‘Hour’-and-‘Amount’"><a href="#2-5-Feature-scaling-with-‘Hour’-and-‘Amount’" class="headerlink" title="2.5: Feature scaling with ‘Hour’ and ‘Amount’"></a>2.5: Feature scaling with ‘Hour’ and ‘Amount’</h4><pre><code class="bash"># Feature scaling with &#39;Amount&#39; and &#39;Hour&#39;
col = [&#39;Amount&#39;,&#39;Hour&#39;]
from sklearn.preprocessing import StandardScaler
sc =StandardScaler()
data_new[col] =sc.fit_transform(data_new[col])
data_new.head()
</code></pre>
<h4 id="2-6-Exploring-feature-importance"><a href="#2-6-Exploring-feature-importance" class="headerlink" title="2.6: Exploring feature importance"></a>2.6: Exploring feature importance</h4><div align="center"><br><img src="./q26.png" width="385" height="587" alt="图片名称" align="center"><br></div><br><code>bash
#导入库
from sklearn.ensemble import RandomForestClassifier
import plotly.offline as py
py.init_notebook_mode(connected=True)
import plotly.graph_objs as go
import plotly.tools as tls

X_columns=data_new.drop([&#39;Class&#39;], axis=1).columns
X_value= data_new.drop([&#39;Class&#39;], axis=1)
y_value=data_new[&#39;Class&#39;]
rf=RandomForestClassifier()

#训练模型
rf.fit(X_value,y_value)

#预测特征重要度
rf_features =rf.feature_importances_

cols = X_columns.values
feature_dataframe = pd.DataFrame( {&#39;features&#39;: cols,&#39;Random Forest feature importances&#39;: rf_features})
feature_dataframe.sort_values(by=&#39;Random Forest feature importances&#39;,ascending=False)</code><br><br>### 3:Splitting the Data (Whole DataFrame)<br>Before proceeding with the Random UnderSampling technique we have to separate the orginal dataframe.<br>Why? for testing purposes, we want to test our models on the original testing set not on the testing set created by either of UnderSampling or Oversampling techniques. The main goal is to fit the model either with the dataframes that were undersample and oversample (in order for our models to detect the patterns), and test it on the original testing set.<br><br><code>bash
from sklearn.model_selection import train_test_split

X = data_new.drop(&#39;Class&#39;,axis=1)
y = data_new[ &#39;Class&#39;]
# Whole dataset
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 0)

print(&quot;Number transactions train dataset: &quot;, len(X_train))
print(&quot;Number transactions test dataset: &quot;, len(X_test))
print(&quot;Total number of transactions: &quot;, len(X_train)+len(X_test))

-------------------------------------------
Number transactions train dataset:  199364
Number transactions test dataset:  85443
Total number of transactions:  284807</code><br><br>### 4: Random Under-Sampling<br><br><div align="center"><br><img src="./q4.png" width="392" height="279" alt="图片名称" align="center"><br></div>

<pre><code class="bash">#Random undersampling
data_new = data_new.sample(frac=1)

# amount of fraud classes 492 rows.
fraud_df = data_new.loc[data_new[&#39;Class&#39;] == 1]
non_fraud_df = data_new.loc[data_new[&#39;Class&#39;] == 0][:492]

normal_distributed_df = pd.concat([fraud_df, non_fraud_df])

# Shuffle dataframe rows
new_df = normal_distributed_df.sample(frac=1, random_state=42)

X_undersample=new_df.drop([&#39;Class&#39;], axis=1)
y_undersample=new_df[&#39;Class&#39;]
X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample
= train_test_split(X_undersample,y_undersample
,test_size = 0.3,random_state = 42)
print(&quot;&quot;)
print(&quot;Number transactions train dataset: &quot;, len(X_train_undersample))
print(&quot;Number transactions test dataset: &quot;, len(X_test_undersample))
print(&quot;Total number of transactions: &quot;, len(X_train_undersample)+len(X_test_undersample))

--------------
Number transactions train dataset:  688
Number transactions test dataset:  296
Total number of transactions:  984
</code></pre>
<h3 id="5-Logistic-regression-classifier-Skewed-data"><a href="#5-Logistic-regression-classifier-Skewed-data" class="headerlink" title="5:Logistic regression classifier - Skewed data"></a>5:Logistic regression classifier - Skewed data</h3><p>In this phrase, we will use the train and test data from the original skewed dataset. Our intuition is that skewness will introduce issues difficult to capture, and therefore, provide a less effective algorithm.</p>
<blockquote>
<p>Recall metric in the testing dataset:  0.61</p>
</blockquote>
<pre><code class="bash">log_reg_params = {&quot;penalty&quot;: [&#39;l1&#39;, &#39;l2&#39;], &#39;C&#39;: [0.001, 0.01, 0.1, 1, 10, 100, 1000]}

param_grid = {&#39;C&#39;: [0.01,0.1, 1, 10, 100, 1000,],&#39;penalty&#39;: [ &#39;l1&#39;, &#39;l2&#39;]}
grid_search = GridSearchCV(LogisticRegression(),  param_grid, cv=10)
grid_search.fit(X_train, y_train)
print(&quot;Best parameters: {}&quot;.format(grid_search.best_params_))
print(&quot;Best cross-validation score: {:.5f}&quot;.format(grid_search.best_score_))
y_pred = grid_search.predict(X_test)
print(&quot;Test set accuracy score: {:.5f}&quot;.format(accuracy_score(y_test, y_pred)))
labels = [&#39;Non-fraud&#39;,&#39;fraud&#39;]
print(classification_report(y_test, y_pred,target_names=labels))
matrix=confusion_matrix(y_test,y_pred)

---------------------
Best parameters: {&#39;C&#39;: 0.1, &#39;penalty&#39;: &#39;l1&#39;}
Best cross-validation score: 0.99914
Test set accuracy score: 0.99923
             precision    recall  f1-score   support

  Non-fraud       1.00      1.00      1.00     85302
      fraud       0.89      0.61      0.72       141

avg / total       1.00      1.00      1.00     85443

</code></pre>
<div align="center"><br><img src="./q51.png" width="389" height="294" alt="图片名称" align="center"><br></div>


<h4 id="6-Logistic-regression-classifier-Undersampled-data"><a href="#6-Logistic-regression-classifier-Undersampled-data" class="headerlink" title="6:Logistic regression classifier - Undersampled data"></a>6:Logistic regression classifier - Undersampled data</h4><p>In this phrase, we will use train and test data from the Undersampled data. We are very interested in the recall score, because that is the metric that will help us try to capture the most fraudulent transactions.</p>
<blockquote>
<p>Recall metric in the testing dataset:  0.85</p>
</blockquote>
<pre><code class="bash">
from sklearn.model_selection import GridSearchCV

log_reg_params = {&quot;penalty&quot;: [&#39;l1&#39;, &#39;l2&#39;], &#39;C&#39;: [0.001, 0.01, 0.1, 1, 10, 100, 1000]}
param_grid = {&#39;C&#39;: [0.01,0.1, 1, 10, 100, 1000,],&#39;penalty&#39;: [ &#39;l1&#39;, &#39;l2&#39;]}
# Use GridSearchCV to find the best parameters.
grid_search = GridSearchCV(LogisticRegression(),  param_grid, cv=10)
grid_search.fit(X_train_undersample, y_train_undersample)
print(&quot;Best parameters: {}&quot;.format(grid_search.best_params_))
print(&quot;Best cross-validation score: {:.5f}&quot;.format(grid_search.best_score_))
y_pred = grid_search.predict(X_test_undersample)
print(&quot;Test set accuracy score: {:.5f}&quot;.format(accuracy_score(y_test_undersample, y_pred)))
labels = [&#39;Non-fraud&#39;,&#39;fraud&#39;]
print(classification_report(y_test_undersample, y_pred,target_names=labels))
print(confusion_matrix(y_test_undersample,y_pred))
matrix=confusion_matrix(y_test_undersample,y_pred)

-------------------------
Best parameters: {&#39;C&#39;: 10, &#39;penalty&#39;: &#39;l1&#39;}
Best cross-validation score: 0.94477
Test set accuracy score: 0.90541
             precision    recall  f1-score   support

  Non-fraud       0.84      0.97      0.90       134
      fraud       0.97      0.85      0.91       162

avg / total       0.91      0.91      0.91       296

</code></pre>
<div align="center"><br><img src="./q6.png" width="389" height="294" alt="图片名称" align="center"><br></div>

<h4 id="7-Logistic-regression-classifier-use-Undersampled-data-for-fitting-and-original-test-data-for-testing"><a href="#7-Logistic-regression-classifier-use-Undersampled-data-for-fitting-and-original-test-data-for-testing" class="headerlink" title="7:Logistic regression classifier - use Undersampled data for fitting and original test data for testing"></a>7:Logistic regression classifier - use Undersampled data for fitting and original test data for testing</h4><blockquote>
<p>Recall metric in the testing dataset:  0.91</p>
</blockquote>
<p>We found a very decent recall accuracy when applying it to a much larger and skewed dataset. Also, as test on the larger and skewed data we will found a decrease of precision. That’s true, because this time there are more non-fraud transations in testing data. Since you want to identify all fraudulent data resp. At least raise all suspicious data which is probably at loss of precision. You are still reducing the amount which may has to be reviewed manually to tiny part of original transaction data. In our case, if we predict that a transaction is fraudulent and turns out not to be, is not a massive problem compared to the opposite.</p>
<pre><code class="bash">log_reg_params = {&quot;penalty&quot;: [&#39;l1&#39;, &#39;l2&#39;], &#39;C&#39;: [0.001, 0.01, 0.1, 1, 10, 100, 1000]}

param_grid = {&#39;C&#39;: [0.01,0.1, 1, 10, 100, 1000,],&#39;penalty&#39;: [ &#39;l1&#39;, &#39;l2&#39;]}
grid_search = GridSearchCV(LogisticRegression(),  param_grid, cv=10)
grid_search.fit(X_train_undersample, y_train_undersample)
print(&quot;Best parameters: {}&quot;.format(grid_search.best_params_))
print(&quot;Best cross-validation score: {:.5f}&quot;.format(grid_search.best_score_))
y_pred = grid_search.predict(X_test)
print(&quot;Test set accuracy score: {:.5f}&quot;.format(accuracy_score(y_test, y_pred)))
labels = [&#39;Non-fraud&#39;,&#39;fraud&#39;]
print(classification_report(y_test, y_pred,target_names=labels))
print(confusion_matrix(y_test,y_pred))
matrix=confusion_matrix(y_test,y_pred)

-----------------------------------------
Best parameters: {&#39;C&#39;: 10, &#39;penalty&#39;: &#39;l1&#39;}
Best cross-validation score: 0.94477
Test set accuracy score: 0.97795
             precision    recall  f1-score   support

  Non-fraud       1.00      0.98      0.99     85302
      fraud       0.06      0.91      0.12       141

avg / total       1.00      0.98      0.99     85443
</code></pre>
<div align="center"><br><img src="./q7.png" width="389" height="294" alt="图片名称" align="center"><br></div>

            </div>

            <!-- Post Comments -->
            
    <!-- 使用 valine -->
<div id="comment">
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
<script>
    new Valine({
        el: '#comment' ,
        notify: true,
        verify: false,
        app_id: '5ieD2qBIigoLuVa78tzFJ22P-gzGzoHsz',
        app_key: '6TREdEvvBJpk2ElJDUL27w5U',
        placeholder: 'Please leave your footprints',
        pageSize: '10',
        avatar: '',
        avatar_cdn: 'https://gravatar.loli.net/avatar/'
    });
</script>
</div>
<style>
   #comment{
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
            </ul>
            
            	<span id="busuanzi_container_site_pv">2018总访问量<span id="busuanzi_value_site_pv"></span>次</span>
			
        </div>
    </div>
</body>



 	
</html>
