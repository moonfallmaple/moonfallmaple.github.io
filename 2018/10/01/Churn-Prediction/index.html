<!DOCTYPE HTML>
<html>

<head>
	<link rel="bookmark"  type="image/x-icon"  href="/img/logo.png"/>
	<link rel="shortcut icon" href="/img/logo.png">
	
			    <title>
    月落丹枫
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="/css/mic_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
    <meta name="keywords" content="" />
    
    	<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	 
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('/img/bg.jpg') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
</head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_prism_okaidia.css" />
<link rel="stylesheet" href="/css/typo.css" />
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">月落丹枫</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow">Home</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <li class="active">
	            <a href="#s1">Category</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="category-link" href="/categories/Data-Analysis/">Data-Analysis</a>
	                    </ul>
	        </li>
	        
	        <!-- archives  归档   --> 
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/tags/" title="Tag">
		                Tag
		            </a>
		        </li>
		        
		        <li>
		            <a href="/demo/" title="Demo">
		                Demo
		            </a>
		        </li>
		        
		        <li>
		            <a href="https://www.linkedin.com/in/jane-shan/" title="Resume">
		                Resume
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="https://github.com/moonfallmaple" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main" >
            <div class ="post_page_title_img" style="height:8rem;background-image: url();background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 2rem 4rem 2rem 4rem ;"><h2 >Customer churn Prediction</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 2rem 4rem 2rem 4rem ;">
                <h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview:"></a>Overview:</h4><p>This report presents a recommended predictive model for identifying customer with the greatest risk of defaulting from QWE inc’s solution services over the next two months(after  February 2012). This model also presented the top three drivers of churn for the identified customers. The model was built from a RandomForestClassifier using the following information to predict a customer’s likelihood to leave the company:</p>
<blockquote>
<p>Customer age</p>
</blockquote>
<blockquote>
<p>CHI Score Month 0</p>
</blockquote>
<blockquote>
<p>Days Since Last Login 0-1</p>
</blockquote>
<h3 id="Q1-What-are-the-two-important-predictors-of-customer-churn"><a href="#Q1-What-are-the-two-important-predictors-of-customer-churn" class="headerlink" title="Q1 What are the two important predictors of customer churn"></a>Q1 What are the two important predictors of customer churn</h3><div align="center"><br><img src="./q1.png" width="900" height="350" alt="图片名称" align="center"><br></div>

<div align="center"><br><img src="./q11.png" width="700" height="450" alt="图片名称" align="center"><br></div>

<blockquote>
<p>code for calculating and Visualizing feature importance</p>
</blockquote>
<pre><code class="bash">import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt

# Classifier Libraries
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier,
GradientBoostingClassifier,ExtraTreesClassifier)
from xgboost.sklearn import XGBClassifier

# Model selection &amp; evaluation
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score,
 accuracy_score, classification_report
from sklearn.metrics import confusion_matrix


# 1.Import dataset
data=pd.read_excel(&#39;Churn_Case.xlsx&#39;,&#39;Case Data&#39;)

# 2.Rename column name
data=data.rename(index=str, columns={&#39;Churn (1 = Yes, 0 = No)&#39;:&#39;Churn&#39;,&#39;Customer
Age (in months)&#39;:&#39;Customer Age&#39;})

# 3.Drop customer ID
data=data.drop(&#39;ID&#39;,axis=1)

# 4.Feature importance
#import lib
from xgboost import XGBClassifier
from xgboost import plot_importance
from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier,
GradientBoostingClassifier,ExtraTreesClassifier)

#plot
from matplotlib import pyplot
import plotly.offline as py
py.init_notebook_mode(connected=True)
import plotly.graph_objs as go
import plotly.tools as tls

# Initiate model
rf=RandomForestClassifier()
et=ExtraTreesClassifier()
ada=AdaBoostClassifier()
gb=GradientBoostingClassifier()
xg=XGBClassifier()

# split data into X and y
X= data.drop([&#39;Churn&#39;], axis=1)
y=data[&#39;Churn&#39;]
X_columns=data.drop([&#39;Churn&#39;], axis=1).columns.values


#train model
rf.fit(X,y)
et.fit(X,y)
ada.fit(X,y)
gb.fit(X,y)
xg.fit(X,y)

# Get feature importance
rf_features =rf.feature_importances_
et_features = et.feature_importances_
ada_features = ada.feature_importances_
gb_features = gb.feature_importances_
xg_features=xg.feature_importances_

# create feature importance dataframe
feature_dataframe = pd.DataFrame( {
     &#39;Random Forest feature importances&#39;: rf_features,
     &#39;Extra Trees  feature importances&#39;: et_features,
     &#39;AdaBoost feature importances&#39;: ada_features,
     &#39;xg Boost feature importances&#39;: xg_features,
     &#39;Gradient Boost feature importances&#39;: gb_features,
     &#39;features&#39;: X_columns
})

# create average feature importance dataframe
feature_dataframe[&#39;mean&#39;] = feature_dataframe.mean(axis= 1)
feature_dataframe.sort_values(by=&#39;mean&#39;,ascending=False)
</code></pre>
<pre><code class="bash">y = feature_dataframe[&#39;mean&#39;].values

x = feature_dataframe[&#39;features&#39;].values
data = [go.Bar(
            x= x,
             y= y,
            width = 0.5,
            marker=dict(
               color = feature_dataframe[&#39;mean&#39;].values,
            colorscale=&#39;Portland&#39;,
            showscale=True,
            reversescale = False
            ),
            opacity=0.6
        )]

layout= go.Layout(
    autosize= True,
    title= &#39;Barplots of Mean Feature Importance&#39;,
    hovermode= &#39;closest&#39;,
#     xaxis= dict(
#         title= &#39;Pop&#39;,
#         ticklen= 5,
#         zeroline= False,
#         gridwidth= 2,
#     ),
    yaxis=dict(
        title= &#39;Feature Importance&#39;,
        ticklen= 5,
        gridwidth= 2
    ),
    showlegend= False
)
fig = go.Figure(data=data, layout=layout)
py.iplot(fig, filename=&#39;bar-direct-labels&#39;)

</code></pre>
<h3 id="Q2-What-is-the-natural-customer-segmentation-with-the-churn-risk"><a href="#Q2-What-is-the-natural-customer-segmentation-with-the-churn-risk" class="headerlink" title="Q2 What is the natural customer segmentation with the churn risk?"></a>Q2 What is the natural customer segmentation with the churn risk?</h3><h4 id="2-1-Check-the-Monthly-Churn-rate-by-Customer-age"><a href="#2-1-Check-the-Monthly-Churn-rate-by-Customer-age" class="headerlink" title="2.1 Check the Monthly Churn rate by Customer age"></a>2.1 Check the Monthly Churn rate by Customer age</h4><pre><code class="bash">data[[&#39;Churn&#39;,&#39;Customer Age&#39;]].groupby(&#39;Customer Age&#39;).mean().plot.bar(figsize=(15,5),
title=&#39;Monthly Churn rate by Customer age&#39;)
</code></pre>
<div align="center"><br><img src="./q2.png" width="1000" height="350" alt="图片名称" align="center"><br></div>

<h4 id="2-2-Churn-Rate-by-age-segmentation-Whole-data-Mid-7-16-gt-Old-16-67-gt-Young-0-7"><a href="#2-2-Churn-Rate-by-age-segmentation-Whole-data-Mid-7-16-gt-Old-16-67-gt-Young-0-7" class="headerlink" title="2.2 Churn Rate by age segmentation (Whole data) -Mid:(7-16]&gt; Old:(16-67]&gt;Young:(0-7]"></a>2.2 Churn Rate by age segmentation (Whole data) -Mid:(7-16]&gt; Old:(16-67]&gt;Young:(0-7]</h4><blockquote>
<p> we could divide the customer age to 3 categories based on quantiles : young:(0-7], mid:(7-16],old:(16-67]</p>
</blockquote>
<blockquote>
<p>pd.qcut():Discretize variable into equal-sized buckets based on rank or based on sample quantiles.</p>
</blockquote>
<blockquote>
<p>Create a new feature ‘CategoricalAge’ and based on the new feature to calculate the churn rate for each age segmentation.</p>
</blockquote>
<blockquote>
<p>The mid group has the highest percentage to churn, next one is old group,then the young group</p>
</blockquote>
<pre><code class="bash">data[&#39;CategoricalAge&#39;]= pd.qcut(data[&#39;Customer Age&#39;], 3)
print(data[[&#39;CategoricalAge&#39;, &#39;Churn&#39;]].groupby([&#39;CategoricalAge&#39;], as_index=False).mean())

CategoricalAge     Churn
0   (0.999, 7.0]  0.025552
1    (7.0, 16.0]  0.080366
2   (16.0, 67.0]  0.051183
</code></pre>
<div align="center"><br><img src="./q22.png" width="381" height="248" alt="图片名称" align="center"><br></div>

<h4 id="2-3-Churn-Rate-by-age-segmentation-churn-1-Young-0-999-7-12-gt-Old-16-47-gt-Mid-12-16"><a href="#2-3-Churn-Rate-by-age-segmentation-churn-1-Young-0-999-7-12-gt-Old-16-47-gt-Mid-12-16" class="headerlink" title="2.3 Churn Rate by age segmentation (churn=1) -Young:(0.999-7-12]&gt; Old:(16-47]&gt;Mid:(12-16]"></a>2.3 Churn Rate by age segmentation (churn=1) -Young:(0.999-7-12]&gt; Old:(16-47]&gt;Mid:(12-16]</h4><blockquote>
<p>In the subset data (Churn==1),based on quantiles to divede the age for 3 parts :(0.999, 12.0] ,(12.0, 16.0] ,(16.0, 47.0]</p>
</blockquote>
<blockquote>
<p>we found month 12 not only occupies large percentage of the customer age,but also is the time most customer choose to churn, there are 56 customers left in this month.</p>
</blockquote>
<pre><code class="bash">pd.qcut(data[data[&#39;Churn&#39;]==1][&#39;Customer Age&#39;],3)
plt.figure(figsize=(15,5))
data[data[&#39;Churn&#39;]==1][&#39;Customer Age&#39;].value_counts().plot.bar()
</code></pre>
<div align="center"><br><img src="./q23.png" width="874" height="309" alt="图片名称" align="center"><br></div>

<blockquote>
<p>The Young group has the highest percentage to churn, next one is old group,then the Mid group</p>
</blockquote>
<pre><code class="bash">print(&#39;Young group:&#39;,round(len(data[(data[&#39;Churn&#39;]==1)&amp;(data[&#39;Customer Age&#39;]&lt;=12)
&amp;(data[&#39;Customer Age&#39;]&gt;0.999)])/(len(data[&#39;Churn&#39;]==1)),4))
print(&#39;Mid group:&#39;,round(len(data[(data[&#39;Churn&#39;]==1)&amp;(data[&#39;Customer Age&#39;]&lt;=16)
&amp;(data[&#39;Customer Age&#39;]&gt;12)])/(len(data[&#39;Churn&#39;]==1)),4))
print(&#39;Old group:&#39;,round(len(data[(data[&#39;Churn&#39;]==1)&amp;(data[&#39;Customer Age&#39;]&lt;=47)
&amp;(data[&#39;Customer Age&#39;]&gt;16)])/(len(data[&#39;Churn&#39;]==1)),4))

Young group: 0.0244
Mid group: 0.0098
Old group: 0.0167
</code></pre>
<h4 id="2-4-Age-segmentation-Conculsion"><a href="#2-4-Age-segmentation-Conculsion" class="headerlink" title="2.4 Age segmentation Conculsion:"></a>2.4 Age segmentation Conculsion:</h4><blockquote>
<p>1) We could found 16 are division point in both two cuts.</p>
</blockquote>
<blockquote>
<p>2) Since month 12 has the maximum churn number, it will be an important month, If we put in the first group then first group will be the most risk group, If we put in the second group then second group will be most risk group. I suggest to put in the second group, use the age cut young:(0-7], mid:(7-16],old:(16-67]</p>
</blockquote>
<blockquote>
<p>3) Reason: for a small and medium size business like qwe (provide online subscription services), customer who use service for 12 month is a long time,should pay more attention to them. So, it’s better to seperate them with new customers</p>
</blockquote>
<h3 id="Q3-Build-a-model-to-find-out-the-small-subset-of-particularly-risky-customers"><a href="#Q3-Build-a-model-to-find-out-the-small-subset-of-particularly-risky-customers" class="headerlink" title="Q3 Build a model to find out the small subset of particularly risky customers"></a>Q3 Build a model to find out the small subset of particularly risky customers</h3><h4 id="3-1-Splitting-the-Data"><a href="#3-1-Splitting-the-Data" class="headerlink" title="3.1 Splitting the Data"></a>3.1 Splitting the Data</h4><p>Before proceeding with the Random UnderSampling technique we have to separate the orginal dataframe. Why? for testing purposes, remember although we are splitting the data when implementing Random UnderSampling or OverSampling techniques, we want to test our models on the original testing set not on the testing set created by either of these techniques. The main goal is to fit the model either with the dataframes that were undersample and oversample (in order for our models to detect the patterns), and test it on the original testing set.</p>
<pre><code class="bash">X = data.drop(&#39;Churn&#39;, axis=1)
y = data[&#39;Churn&#39;]

sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)

for train_index, test_index in sss.split(X, y):
    print(&quot;Train:&quot;, train_index, &quot;Test:&quot;, test_index)
    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]
    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]

# original_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(X, y
,test_size=0.2, random_state=42)


# See if both the train and test label distribution are similarly distributed
train_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)
test_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)
print(&#39;-&#39; * 100)

print(&#39;Label Distributions: \n&#39;)
print(train_counts_label/ len(original_ytrain))
print(test_counts_label/ len(original_ytest))

#Output
Train: [2791 1260 1386 ... 5933 2387 4736] Test: [5613  835  701 ... 6004 4180 5869]
Train: [3868 3385 2151 ...  768 2401   46] Test: [3257 5589 5299 ... 6140 3425 2872]
Train: [1979  595   77 ...  391  845 5531] Test: [5762 1017 2704 ... 4026 3217 5007]
Train: [   3 3003 2026 ... 3527  819  859] Test: [4126 5827 2926 ...  983 5226  796]
Train: [1995 5377 1192 ... 4003 1082 2392] Test: [5272 1151 1336 ... 1038 1299 3217]
----------------------------------------------------------------------------------------------------
Label Distributions:

[0.94917258 0.05082742]
[0.9488189 0.0511811]
</code></pre>
<h4 id="3-2-Random-Undersampling"><a href="#3-2-Random-Undersampling" class="headerlink" title="3.2 Random Undersampling"></a>3.2 Random Undersampling</h4><p>In this phase of the project we will implement “Random Under Sampling” which basically consists of removing data in order to have a more balanced dataset and thus avoiding our models to overfitting.</p>
<blockquote>
<p>Steps:</p>
</blockquote>
<blockquote>
<p>1) The first thing we have to do is determine how imbalanced is our class (use “value_counts()” on the Churn column to determine the amount for each label)</p>
</blockquote>
<pre><code class="bash">data[&#39;Churn&#39;].value_counts().plot(kind=&#39;bar&#39;)
plt.show()

# Churn is the target column, it has two value 0 and 1.
#(Churn = &quot;1&quot;) means customer acturally left in the two months after Dec 1st
#(Churn = &quot;0&quot;) means customer acturally continue use the service.

# Churn has a large imbalance between it&#39;s two value
0 ： 6024

1 ：323
</code></pre>
<div align="center"><br><img src="./q31.png" width="384" height="248" alt="图片名称" align="center"><br></div>

<blockquote>
<p>2) Once we determine how many instances are considered Churn customers (Churn = “1”) , we should bring the<br>Non-churn customers (Churn = “0”) to the same amount as  Churn customers (assuming we want a 50/50 ratio), this will be equivalent to 323 cases of Churn and 323 cases of Non-churn.</p>
</blockquote>
<blockquote>
<p>3) After implementing this technique, we have a sub-sample of our dataframe with a 50/50 ratio with regards to our classes. Then the next step we will implement is to shuffle the data to see if our models can maintain a certain accuracy everytime we run this script.</p>
</blockquote>
<pre><code class="bash"># Since our classes are highly skewed we should make them equivalent in order to have
a normal distribution of the classes.

# Lets shuffle the data before creating the subsamples
new_df= data[[&#39;CHI Score 0-1&#39;,&#39;CHI Score Month 0&#39;,&#39;Views 0-1&#39;,&#39;Customer Age&#39;,
&#39; Days Since Last Login 0-1&#39;,&#39;Churn&#39;]].sample(frac=1)

# Take same amount (323 rows) of churn and non-churn cases.
Left_df = new_df.loc[new_df[&#39;Churn&#39;] == 1]
Stay_df = new_df.loc[new_df[&#39;Churn&#39;] == 0][:323]

normal_distributed_df = pd.concat([Left_df, Stay_df])

# Shuffle dataframe rows
new_df = normal_distributed_df.sample(frac=1, random_state=42)

</code></pre>
<div align="center"><br><img src="./q32.png" width="392" height="279" alt="图片名称" align="center"><br></div>

<blockquote>
<p>Note: The main issue with “Random Under-Sampling” is that we run the risk that our classification models will not perform as accurate as we would like to since there is a great deal of information loss (bringing 323 Churn=0 customers from 6,024 Churn=0 customers)</p>
</blockquote>
<h4 id="3-3-Test-Data-with-RandomForestClassifier"><a href="#3-3-Test-Data-with-RandomForestClassifier" class="headerlink" title="3.3 Test Data with RandomForestClassifier:"></a>3.3 Test Data with RandomForestClassifier:</h4><p>Random UnderSampling: We will evaluate the final performance of the classification models in the original_ytest<br>Classification Models: The models that performed the best were RandomForestClassifier</p>
<blockquote>
<p>Recall metric in the testing dataset:  0.95</p>
</blockquote>
<pre><code class="bash">new_X = new_df.drop(&#39;Churn&#39;, axis=1).values
new_y = new_df[&#39;Churn&#39;].values

new_X_train, new_X_test, new_y_train, new_y_test = train_test_split(new_X, new_y,
test_size=0.2, random_state=42)

#RandomForestClassifier
tree =RandomForestClassifier()
tree.fit(new_X_train, new_y_train)

y_pred = tree.predict(original_Xtest)
print(&quot;Test set accuracy score: {:.5f}&quot;.format(accuracy_score(y_pred,original_ytest)))
print(classification_report(original_ytest, y_pred, target_names=labels))
print(confusion_matrix(y_pred,original_ytest))

#output
Test set accuracy score: 0.76220
             precision    recall  f1-score   support

    Churn=0       1.00      0.75      0.86      1205
    Churn=1       0.17      0.95      0.29        65

avg / total       0.95      0.76      0.83      1270


</code></pre>
<blockquote>
<p>Confusion Matrix:</p>
</blockquote>
<p>Positive/Negative: Type of Class (label) [‘Churn=1’,’Churn=0’]<br>True/False: Correctly or Incorrectly classified by the model.</p>
<p>True Negatives (Top-Left Square): This is the number of correctly classifications of the “Churn=0” (Non-Churn customers) class.</p>
<p>False Negatives (Top-Right Square): This is the number of incorrectly classifications of the “Churn=0” (Non-Churn customers) class.</p>
<p>False Positives (Bottom-Left Square): This is the number of incorrectly classifications of the “Churn=1” (Churn customers) class.</p>
<p>True Positives (Bottom-Right Square): This is the number of correctly classifications of the “Churn=1” (Churn customers) class .</p>
<div align="center"><br><img src="./q33.png" width="381" height="300" alt="图片名称" align="center"><br></div>

<blockquote>
<p>Code for the confusion matrix:</p>
</blockquote>
<pre><code class="bash">import itertools
def plot_confusion_matrix(cm, classes,
                          title=&#39;Confusion matrix&#39;,
                          cmap=plt.cm.Blues):
    &quot;&quot;&quot;
    This function prints and plots the confusion matrix.
    &quot;&quot;&quot;
    plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=0)
    plt.yticks(tick_marks, classes)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment=&quot;center&quot;,
                 color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;)

    plt.tight_layout()
    plt.ylabel(&#39;True label&#39;)
    plt.xlabel(&#39;Predicted label&#39;)

labels = [&#39;Churn=0&#39;, &#39;Churn=1&#39;]
plot_confusion_matrix(tree_matrix
                      , classes=labels
                      , title=&#39;Confusion matrix&#39;)
plt.show()

</code></pre>
<h3 id="Q4-what-characteristics-separate-them-from-the-rest-of-QWE-customers"><a href="#Q4-what-characteristics-separate-them-from-the-rest-of-QWE-customers" class="headerlink" title="Q4 what characteristics separate them from the rest of QWE customers?"></a>Q4 what characteristics separate them from the rest of QWE customers?</h3><p>From the following decision tree, We can see the top 3 characteristics for customer Churn:<br>If we use these conditions to filter in the  original dataset (6346 rows ) we can get 350 customers<br>includes 279 (Churn=0) and 71(churn=1).</p>
<blockquote>
<p>11.5=&lt; Customer Age &lt;=41</p>
</blockquote>
<blockquote>
<p>CHI Score Month 0 &lt;=73.5</p>
</blockquote>
<blockquote>
<p>Days Since Last Login 0-1 &gt;=23.5</p>
</blockquote>
<pre><code class="bash">datamask=data[(data[&#39;Customer Age&#39;]&gt;=11.5)&amp;(data[&#39;Customer Age&#39;]&lt;=41)&amp;
(data[&#39;CHI Score Month 0&#39;]&lt;=73.5)&amp;(data[&#39; Days Since Last Login 0-1&#39;]&gt;=23.5)]
datamask[&#39;Churn&#39;].value_counts()

0    279
1     71
Name: Churn, dtype: int64
</code></pre>
<div align="center"><br><img src="./q42.png" width="1245" height="841" alt="图片名称" align="center"><br></div>

<blockquote>
<p>Code for Visualizing a Decision Tree from a Random Forest :</p>
</blockquote>
<pre><code class="bash">feature_names=new_df.drop(&#39;Churn&#39;,axis=1).columns
target_names= [&#39;Churn=0&#39;, &#39;Churn=1&#39;]

from sklearn import tree
import pydotplus
# Model (can also use single decision tree)
from sklearn.ensemble import RandomForestClassifier
model =RandomForestClassifier(max_depth=5, criterion=&#39;gini&#39;)

# Train
model.fit(new_X_train, new_y_train)
# Extract single tree
estimator = model.estimators_[5]

from sklearn.tree import export_graphviz
# Export as dot file
dot_data = tree.export_graphviz(estimator, out_file=None,
                feature_names = feature_names,
                class_names = target_names,
                rounded = True, proportion = False,
                precision = 2, filled = True)

# Convert to png using system command (requires Graphviz)
from subprocess import call
call([&#39;dot&#39;, &#39;-Tpng&#39;, &#39;tree.dot&#39;, &#39;-o&#39;, &#39;tree.png&#39;, &#39;-Gdpi=600&#39;])
# Draw graph
graph = pydotplus.graph_from_dot_data(dot_data)  

# Display in jupyter notebook
from IPython.display import Image
Image(graph.create_png())
</code></pre>
<h3 id="Q5-Can-our-Model-support-a-call-based-program-to-improve-customer-retention"><a href="#Q5-Can-our-Model-support-a-call-based-program-to-improve-customer-retention" class="headerlink" title="Q5 Can our Model support a call-based program to improve customer retention?"></a>Q5 Can our Model support a call-based program to improve customer retention?</h3><p>From the above step, we use 3 conditions to reduce the risky customers to 350.<br>Then we can random select 100 customers from those customers. And we repeat selection for 30 times.<br>The mean Churn rate for 100 customers will be close : 20%<br>Then calculate the expected return per hundred customers</p>
<div align="center"><br><img src="./q51.png" width="539" height="198" alt="图片名称" align="center"><br></div>

<p>We get $1000 per hundred customers, it shows our model can support this call_based program.</p>
<pre><code class="bash">score=[]
for i in range(1, 30):
    dfi=datamask.sample(n=100)
    score.append(dfi[:100][&#39;Churn&#39;].value_counts()[1]/100)
print(score)   
print(&quot;The mean Churn rate for 100 customers :&quot;,round(sum(score) / float(len(score)),2))

#Output
[0.2, 0.26, 0.2, 0.2, 0.2, 0.25, 0.17, 0.23, 0.17, 0.29, 0.22, 0.21, 0.24, 0.16,
0.22, 0.2, 0.15, 0.24, 0.19, 0.2, 0.25, 0.18, 0.2, 0.2, 0.14, 0.19, 0.19, 0.21, 0.14]

The mean Churn rate for 100 customers : 0.2
</code></pre>

            </div>

            <!-- Post Comments -->
            
    <!-- 使用 valine -->
<div id="comment">
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
<script>
    new Valine({
        el: '#comment' ,
        notify: true,
        verify: false,
        app_id: '5ieD2qBIigoLuVa78tzFJ22P-gzGzoHsz',
        app_key: '6TREdEvvBJpk2ElJDUL27w5U',
        placeholder: 'Please leave your footprints',
        pageSize: '10',
        avatar: '',
        avatar_cdn: 'https://gravatar.loli.net/avatar/'
    });
</script>
</div>
<style>
   #comment{
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
            </ul>
            
            	<span id="busuanzi_container_site_pv">2019总访问量<span id="busuanzi_value_site_pv"></span>次</span>
			
        </div>
    </div>
</body>



 	
</html>
